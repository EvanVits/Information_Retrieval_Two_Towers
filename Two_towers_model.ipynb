{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OI9y0V6S_Gso",
    "outputId": "722d19aa-208f-4b1d-e924-59d4a7ac702d"
   },
   "outputs": [],
   "source": [
    "# libraries\n",
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler, StandardScaler, MultiLabelBinarizer\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Dot, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uYpqdOGYHUN4"
   },
   "source": [
    "# Load the dataframe from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pG70hnhfIBG_",
    "outputId": "8b4d2315-c4b5-4539-824b-816283a81034"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# make sure the dataframe file is in your google drive and you can then load it\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N48VUyWoRb2l"
   },
   "outputs": [],
   "source": [
    "merged_df = pd.read_parquet(\"/content/drive/MyDrive/merged_df.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9XufdpSSS3i"
   },
   "source": [
    "# If the above cell doesn't work, run this one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BEc_jDgoLoxP",
    "outputId": "1fe76d0b-1d78-4ef6-d676-1b8398383059"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded row group 1/11, shape: (1048576, 51)\n",
      "Loaded row group 2/11, shape: (1048576, 51)\n",
      "Loaded row group 3/11, shape: (1048576, 51)\n",
      "Loaded row group 4/11, shape: (1048576, 51)\n",
      "Loaded row group 5/11, shape: (1048576, 51)\n",
      "Loaded row group 6/11, shape: (1048576, 51)\n",
      "Loaded row group 7/11, shape: (1048576, 51)\n",
      "Loaded row group 8/11, shape: (1048576, 51)\n",
      "Loaded row group 9/11, shape: (1048576, 51)\n",
      "Loaded row group 10/11, shape: (1048576, 51)\n",
      "Loaded row group 11/11, shape: (951877, 51)\n",
      "Final DataFrame shape: (11437637, 51)\n"
     ]
    }
   ],
   "source": [
    "# import pyarrow.parquet as pq\n",
    "# import pandas as pd\n",
    "\n",
    "# file_path = \"/content/drive/MyDrive/merged_df.parquet\"\n",
    "# parquet_file = pq.ParquetFile(file_path)\n",
    "\n",
    "# # Initialize an empty list to store DataFrames\n",
    "# dataframes = []\n",
    "\n",
    "# # Iterate through each row group\n",
    "# for i in range(parquet_file.num_row_groups):\n",
    "#     row_group = parquet_file.read_row_group(i)\n",
    "#     df = row_group.to_pandas()  # Convert row group to Pandas DataFrame\n",
    "#     dataframes.append(df)  # Add the DataFrame to the list\n",
    "#     print(f\"Loaded row group {i + 1}/{parquet_file.num_row_groups}, shape: {df.shape}\")\n",
    "\n",
    "# # Concatenate all row groups into a single DataFrame\n",
    "# merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# # Free up memory\n",
    "# del dataframes\n",
    "\n",
    "# # Display the final DataFrame shape\n",
    "# print(f\"Final DataFrame shape: {merged_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oJHSZ1dyMbu3",
    "outputId": "41fa743c-260f-4e13-ac31-4f33e5c9584f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   userId  movieId  rating  budget original_language          original_title  \\\n",
      "0       1      110     1.0       0                fr  Trois couleurs : Rouge   \n",
      "1      11      110     3.5       0                fr  Trois couleurs : Rouge   \n",
      "2      22      110     5.0       0                fr  Trois couleurs : Rouge   \n",
      "3      24      110     5.0       0                fr  Trois couleurs : Rouge   \n",
      "4      29      110     3.0       0                fr  Trois couleurs : Rouge   \n",
      "\n",
      "   popularity  year  revenue  duration  ... Italiano  Português  Pусский  \\\n",
      "0    7.832755  1994      0.0        99  ...        0          0        0   \n",
      "1    7.832755  1994      0.0        99  ...        0          0        0   \n",
      "2    7.832755  1994      0.0        99  ...        0          0        0   \n",
      "3    7.832755  1994      0.0        99  ...        0          0        0   \n",
      "4    7.832755  1994      0.0        99  ...        0          0        0   \n",
      "\n",
      "   suomi  svenska  العربية  हिन्दी  日本語  普通话  Other_language  \n",
      "0      0        0        0       0    0    0               0  \n",
      "1      0        0        0       0    0    0               0  \n",
      "2      0        0        0       0    0    0               0  \n",
      "3      0        0        0       0    0    0               0  \n",
      "4      0        0        0       0    0    0               0  \n",
      "\n",
      "[5 rows x 47 columns]\n"
     ]
    }
   ],
   "source": [
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2RRAGGGgUjZi",
    "outputId": "75c22ba8-7e0c-4e14-9e0e-cfd1443a9b1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['userId', 'movieId', 'rating', 'budget', 'original_language',\n",
      "       'original_title', 'popularity', 'year', 'revenue', 'duration', 'title',\n",
      "       'vote_average', 'vote_count', 'Action', 'Adventure', 'Animation',\n",
      "       'Comedy', 'Crime', 'Documentary', 'Drama', 'Family', 'Fantasy',\n",
      "       'Foreign', 'History', 'Horror', 'Music', 'Mystery', 'Romance',\n",
      "       'Science Fiction', 'TV Movie', 'Thriller', 'War', 'Western', 'Deutsch',\n",
      "       'English', 'Español', 'Français', 'Italiano', 'Português', 'Pусский',\n",
      "       'suomi', 'svenska', 'العربية', 'हिन्दी', '日本語', '普通话',\n",
      "       'Other_language'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(merged_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HyMD9Rw0zz2a"
   },
   "outputs": [],
   "source": [
    "num_users = merged_df['userId'].nunique()\n",
    "num_movies = merged_df['movieId'].nunique()\n",
    "embedding_size = 8  # Size of the embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XUBGZgFoz1kE"
   },
   "outputs": [],
   "source": [
    "# Define User Tower\n",
    "user_input = Input(shape=(1,), name='user_input')\n",
    "user_embedding = Embedding(input_dim=num_users, output_dim=embedding_size)(user_input)\n",
    "user_vector = Flatten()(user_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r7qrBKNyz4ha"
   },
   "outputs": [],
   "source": [
    "# Define Content Tower\n",
    "movies_input = Input(shape=(1,), name='movies_input')\n",
    "movie_embedding = Embedding(input_dim=num_movies, output_dim=embedding_size)(movies_input)\n",
    "movie_vector = Flatten()(movie_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QAXij99JByis"
   },
   "source": [
    "# First model. Adam optimizer, mean squared error loss and MAE metric. Linear activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_aOa_sdWz6ek"
   },
   "outputs": [],
   "source": [
    "# Combine Towers\n",
    "dot_product = Dot(axes=1)([user_vector, movie_vector])\n",
    "output = Dense(1, activation='linear')(dot_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mmDy7N5hz8Oj"
   },
   "outputs": [],
   "source": [
    "# Build Model\n",
    "model = Model(inputs=[user_input, movies_input], outputs=output)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ljaytO_WvpEZ",
    "outputId": "579afb5f-7b70-467b-c6b6-b34d48ee060f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m35743/35743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1106s\u001b[0m 31ms/step - loss: 2.7518 - mae: 1.2219\n",
      "Epoch 2/10\n",
      "\u001b[1m11506/35743\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m11:40\u001b[0m 29ms/step - loss: 0.6999 - mae: 0.6358"
     ]
    }
   ],
   "source": [
    "user_movie_pairs = merged_df[['userId', 'movieId']].values\n",
    "ratings = (merged_df['rating']).astype(float).values\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(user_movie_pairs, ratings, test_size=0.2, random_state=42)\n",
    "X_train[:, 0] = X_train[:, 0].astype(int)\n",
    "X_train[:, 1] = X_train[:, 1].astype(int)\n",
    "# Train the Model\n",
    "model.fit([X_train[:, 0], X_train[:, 1]], y_train, epochs=10, batch_size=256)\n",
    "\n",
    "# Evaluate the Model\n",
    "loss, mae = model.evaluate([X_test[:, 0], X_test[:, 1]], y_test)\n",
    "print(f\"Test Loss: {loss}, Test MAE: {mae}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ioCyTRJLCCri"
   },
   "source": [
    "# Second model. Adam optimizer, mean squared error loss and MAE metric. Sigmoid activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xfPaNq2UF6Z8"
   },
   "outputs": [],
   "source": [
    "# Combine Towers\n",
    "dot_product = Dot(axes=1)([user_vector, movie_vector])\n",
    "output2 = Dense(1, activation='sigmoid')(dot_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R8k8rU9OF6Z9"
   },
   "outputs": [],
   "source": [
    "# Build Model\n",
    "model2 = Model(inputs=[user_input, movies_input], outputs=output2)\n",
    "model2.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cpVikOU5GJEB"
   },
   "outputs": [],
   "source": [
    "user_movie_pairs = merged_df[['userId', 'movieId']].values\n",
    "ratings = (merged_df['rating']).astype(float).values\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(user_movie_pairs, ratings, test_size=0.2, random_state=42)\n",
    "X_train[:, 0] = X_train[:, 0].astype(int)\n",
    "X_train[:, 1] = X_train[:, 1].astype(int)\n",
    "# Train the Model\n",
    "model2.fit([X_train[:, 0], X_train[:, 1]], y_train, epochs=10, batch_size=256)\n",
    "\n",
    "# Evaluate the Model\n",
    "loss, mae = model2.evaluate([X_test[:, 0], X_test[:, 1]], y_test)\n",
    "print(f\"Test Loss: {loss}, Test MAE: {mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YNg7d3seCV2z"
   },
   "source": [
    "# Third model. Adam optimizer, mean squared error. MAE metric. Tanh activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AgFTmqJbJzPC"
   },
   "outputs": [],
   "source": [
    "# Combine Towers\n",
    "dot_product = Dot(axes=1)([user_vector, movie_vector])\n",
    "output3 = Dense(1, activation='tanh')(dot_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sR6babCrJzPD"
   },
   "outputs": [],
   "source": [
    "# Build Model\n",
    "model3 = Model(inputs=[user_input, movies_input], outputs=output3)\n",
    "model3.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2fStAhIhJzPD"
   },
   "outputs": [],
   "source": [
    "user_movie_pairs = merged_df[['userId', 'movieId']].values\n",
    "ratings = (merged_df['rating']).astype(float).values\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(user_movie_pairs, ratings, test_size=0.2, random_state=42)\n",
    "X_train[:, 0] = X_train[:, 0].astype(int)\n",
    "X_train[:, 1] = X_train[:, 1].astype(int)\n",
    "# Train the Model\n",
    "model3.fit([X_train[:, 0], X_train[:, 1]], y_train, epochs=10, batch_size=256)\n",
    "\n",
    "# Evaluate the Model\n",
    "loss, mae = model3.evaluate([X_test[:, 0], X_test[:, 1]], y_test)\n",
    "print(f\"Test Loss: {loss}, Test MAE: {mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "veYi7T9kCb5s"
   },
   "source": [
    "# Fourth model. Adam optimizer, mean squared error loss, MAE metric. ReLu activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w6q8k9JyJ_lr"
   },
   "outputs": [],
   "source": [
    "# Combine Towers\n",
    "dot_product = Dot(axes=1)([user_vector, movie_vector])\n",
    "output4 = Dense(1, activation='relu')(dot_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a3m37hrQJ_lr"
   },
   "outputs": [],
   "source": [
    "# Build Model\n",
    "model4 = Model(inputs=[user_input, movies_input], outputs=output4)\n",
    "model4.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VHvC3zmTJ_ls"
   },
   "outputs": [],
   "source": [
    "user_movie_pairs = merged_df[['userId', 'movieId']].values\n",
    "ratings = (merged_df['rating']).astype(float).values\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(user_movie_pairs, ratings, test_size=0.2, random_state=42)\n",
    "X_train[:, 0] = X_train[:, 0].astype(int)\n",
    "X_train[:, 1] = X_train[:, 1].astype(int)\n",
    "# Train the Model\n",
    "model4.fit([X_train[:, 0], X_train[:, 1]], y_train, epochs=10, batch_size=256)\n",
    "\n",
    "# Evaluate the Model\n",
    "loss, mae = model4.evaluate([X_test[:, 0], X_test[:, 1]], y_test)\n",
    "print(f\"Test Loss: {loss}, Test MAE: {mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yrfSxYI7CloJ"
   },
   "source": [
    "# Fifth model. SGD optimizer, mean squared error loss, MAE metric. ReLu activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pki4SvMnkk9T"
   },
   "outputs": [],
   "source": [
    "# Combine Towers\n",
    "dot_product = Dot(axes=1)([user_vector, movie_vector])\n",
    "output4 = Dense(1, activation='relu')(dot_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4GuylTszkk9U"
   },
   "outputs": [],
   "source": [
    "# Build Model\n",
    "model5 = Model(inputs=[user_input, movies_input], outputs=output4)\n",
    "model5.compile(optimizer='sgd', loss='mean_squared_error', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K9iC_t2ikk9U"
   },
   "outputs": [],
   "source": [
    "user_movie_pairs = merged_df[['userId', 'movieId']].values\n",
    "ratings = (merged_df['rating']).astype(float).values\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(user_movie_pairs, ratings, test_size=0.2, random_state=42)\n",
    "X_train[:, 0] = X_train[:, 0].astype(int)\n",
    "X_train[:, 1] = X_train[:, 1].astype(int)\n",
    "# Train the Model\n",
    "model5.fit([X_train[:, 0], X_train[:, 1]], y_train, epochs=10, batch_size=256)\n",
    "\n",
    "# Evaluate the Model\n",
    "loss, mae = model5.evaluate([X_test[:, 0], X_test[:, 1]], y_test)\n",
    "print(f\"Test Loss: {loss}, Test MAE: {mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_nhOufxCtb1"
   },
   "source": [
    "# Sixth model. rmsprop optimizer, mean squared error loss, MAE as metric. ReLu activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dfHRYMRPk6F0"
   },
   "outputs": [],
   "source": [
    "# Combine Towers\n",
    "dot_product = Dot(axes=1)([user_vector, movie_vector])\n",
    "output4 = Dense(1, activation='relu')(dot_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FLgPXAY5k6F0"
   },
   "outputs": [],
   "source": [
    "# Build Model\n",
    "model6 = Model(inputs=[user_input, movies_input], outputs=output4)\n",
    "model6.compile(optimizer='rmsprop', loss='mean_squared_error', metrics=['MAE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ftRffmnk6F0"
   },
   "outputs": [],
   "source": [
    "user_movie_pairs = merged_df[['userId', 'movieId']].values\n",
    "ratings = (merged_df['rating']).astype(float).values\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(user_movie_pairs, ratings, test_size=0.2, random_state=42)\n",
    "X_train[:, 0] = X_train[:, 0].astype(int)\n",
    "X_train[:, 1] = X_train[:, 1].astype(int)\n",
    "# Train the Model\n",
    "model6.fit([X_train[:, 0], X_train[:, 1]], y_train, epochs=10, batch_size=256)\n",
    "\n",
    "# Evaluate the Model\n",
    "loss, mae = model6.evaluate([X_test[:, 0], X_test[:, 1]], y_test)\n",
    "print(f\"Test Loss: {loss}, Test MAE: {mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7mGktu6C3nO"
   },
   "source": [
    "# Seventh model. Adagrad optimizer, mean squared error loss, MAE metric. ReLu activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sal6McKclCZL"
   },
   "outputs": [],
   "source": [
    "# Combine Towers\n",
    "dot_product = Dot(axes=1)([user_vector, movie_vector])\n",
    "output4 = Dense(1, activation='relu')(dot_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OLJeiRYllCZM"
   },
   "outputs": [],
   "source": [
    "# Build Model\n",
    "model7 = Model(inputs=[user_input, movies_input], outputs=output4)\n",
    "model7.compile(optimizer='adagrad', loss='mean_squared_error', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VmPsNXZnlCZM"
   },
   "outputs": [],
   "source": [
    "user_movie_pairs = merged_df[['userId', 'movieId']].values\n",
    "ratings = (merged_df['rating']).astype(float).values\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(user_movie_pairs, ratings, test_size=0.2, random_state=42)\n",
    "X_train[:, 0] = X_train[:, 0].astype(int)\n",
    "X_train[:, 1] = X_train[:, 1].astype(int)\n",
    "# Train the Model\n",
    "model7.fit([X_train[:, 0], X_train[:, 1]], y_train, epochs=10, batch_size=256)\n",
    "\n",
    "# Evaluate the Model\n",
    "loss, mae = model7.evaluate([X_test[:, 0], X_test[:, 1]], y_test)\n",
    "print(f\"Test Loss: {loss}, Test MAE: {mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kA3fOzbgDUmV"
   },
   "source": [
    "# Model with features instead of just MovieId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q4u9404m3XXQ"
   },
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dBnx38Ipv6lI"
   },
   "outputs": [],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2pnZe8-jPWLp"
   },
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler, StandardScaler\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Dot, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "\n",
    "\n",
    "# language\n",
    "language_encoder = LabelEncoder()\n",
    "merged_df['language_encoded'] = language_encoder.fit_transform(merged_df['original_language'])\n",
    "\n",
    "# scale year\n",
    "year_scaler = MinMaxScaler()\n",
    "merged_df['year_scaled'] = year_scaler.fit_transform(merged_df[['year']])\n",
    "\n",
    "# scale budget\n",
    "budget_scaler = StandardScaler()\n",
    "merged_df[\"budget_scaled\"] = budget_scaler.fit_transform(merged_df[[\"budget\"]])\n",
    "\n",
    "# scale popularity\n",
    "popularity_scaler = StandardScaler()\n",
    "merged_df[\"popularity_scaled\"] = popularity_scaler.fit_transform(merged_df[[\"popularity\"]])\n",
    "\n",
    "# duration scaling\n",
    "duration_scaler = MinMaxScaler()\n",
    "merged_df[\"duration_scaled\"] = duration_scaler.fit_transform(merged_df[[\"duration\"]])\n",
    "\n",
    "# vote_average\n",
    "vote_average_scaler = StandardScaler()\n",
    "merged_df['vote_average_scaled'] = vote_average_scaler.fit_transform(merged_df[['vote_average']])\n",
    "\n",
    "# vote_count\n",
    "vote_count_scaler = StandardScaler()\n",
    "merged_df['vote_count_scaled'] = vote_count_scaler.fit_transform(merged_df[['vote_count']])\n",
    "\n",
    "revenue_scaler = StandardScaler()\n",
    "merged_df[\"revenue_scaled\"] = revenue_scaler.fit_transform(merged_df[[\"revenue\"]])\n",
    "\n",
    "\n",
    "# Model parameters\n",
    "num_users = merged_df['userId'].nunique()\n",
    "num_movies = merged_df['movieId'].nunique()\n",
    "num_languages = merged_df['language_encoded'].nunique()\n",
    "genre_columns = ['Action', 'Adventure', 'Animation', 'Comedy', 'Crime', 'Documentary',\n",
    "     'Drama', 'Family', 'Fantasy', 'Foreign', 'History', 'Horror', 'Music',\n",
    "     'Mystery', 'Romance', 'Science Fiction', 'TV Movie', 'Thriller', 'War',\n",
    "     'Western']\n",
    "language_columns = ['Deutsch', 'English', 'Español', 'Français', 'Italiano',\n",
    "     'Português', 'Pусский', 'suomi', 'svenska', 'العربية', 'हिन्दी',\n",
    "     '日本語', '普通话', 'Other_language']\n",
    "num_genres = len([col for col in merged_df.columns if col in genre_columns])\n",
    "num_language_columns = len([col for col in merged_df.columns if col in language_columns])\n",
    "embedding_size = 8\n",
    "\n",
    "# Define User Tower\n",
    "user_input = Input(shape=(1,), name='user_input')\n",
    "user_embedding = Embedding(input_dim=num_users, output_dim=embedding_size)(user_input)\n",
    "user_vector = Flatten()(user_embedding)\n",
    "\n",
    "movie_year_input = Input(shape=(1,), name='movie_year_input')  # Year input (normalized)\n",
    "movie_year_dense = Dense(embedding_size, activation='relu')(movie_year_input)\n",
    "\n",
    "main_language_input = Input(shape=(1,), name='main_language_input')\n",
    "main_language_embedding = Embedding(input_dim=num_languages, output_dim=embedding_size)(main_language_input)\n",
    "main_language_vector = Flatten()(main_language_embedding)\n",
    "\n",
    "budget_input = Input(shape=(1,), name=\"movie_budget_input\")\n",
    "budget_dense = Dense(embedding_size, activation='relu')(budget_input)\n",
    "\n",
    "popularity_input = Input(shape=(1,), name=\"movie_popularity_input\")\n",
    "popularity_dense = Dense(embedding_size, activation='relu')(popularity_input)\n",
    "\n",
    "duration_input = Input(shape=(1, ), name=\"movie_duration_input\")\n",
    "duration_dense = Dense(embedding_size, activation='relu')(duration_input)\n",
    "\n",
    "vote_average_input = Input(shape=(1,), name='vote_average_input')\n",
    "vote_average_dense = Dense(embedding_size, activation='relu')(vote_average_input)\n",
    "\n",
    "vote_count_input = Input(shape=(1,), name='vote_count_input')\n",
    "vote_count_dense = Dense(embedding_size, activation='relu')(vote_count_input)\n",
    "\n",
    "revenue_input = Input(shape=(1, ), name=\"revenue_input\")\n",
    "revenue_dense = Dense(embedding_size, activation=\"relu\")(revenue_input)\n",
    "\n",
    "# NOTE: genre and languages are not currently used in the model. We could not get this to work. The input and dense layers we wanted to use are defined below\n",
    "genre_features = merged_df[genre_columns].values\n",
    "language_features = merged_df[language_columns].values\n",
    "\n",
    "genre_input = Input(shape=(num_genres,), name='genre_input')\n",
    "genre_dense = Dense(embedding_size, activation='relu')(genre_input)\n",
    "\n",
    "language_input = Input(shape=(num_language_columns,), name='language_input')\n",
    "language_dense = Dense(embedding_size, activation='relu')(language_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pj06elHhzj4w"
   },
   "outputs": [],
   "source": [
    "# Combine Movie Features\n",
    "movie_features = concatenate([movie_year_dense, movie_language_dense, budget_dense, popularity_dense, duration_dense, vote_average_dense, vote_count_dense, revenue_dense])\n",
    "movie_features_dense = Dense(embedding_size, activation='relu')(movie_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DcYAPdPcPWLq"
   },
   "outputs": [],
   "source": [
    "# Prepare Training Data\n",
    "user_movie_pairs = merged_df[['userId', 'year_scaled','language_encoded', 'budget_scaled', 'popularity_scaled', 'duration_scaled', 'vote_average_scaled', 'vote_count_scaled', 'revenue_scaled']].values\n",
    "ratings = (merged_df['rating']).astype(float).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YLcOUis6PWLr"
   },
   "outputs": [],
   "source": [
    "# Combine Towers\n",
    "dot_product = Dot(axes=1)([user_vector, movie_features_dense])\n",
    "output_final_model = Dense(1, activation='relu')(dot_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C5VmDDTlPWLr"
   },
   "outputs": [],
   "source": [
    "# Build Model\n",
    "model_final = Model(inputs=[user_input, movie_year_input, movie_language_input, budget_input, popularity_input, duration_input, vote_average_input, vote_count_input, revenue_input], outputs=output_final_model)\n",
    "model_final.compile(optimizer='adam', loss='mse', metrics=['MAE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RjkmHh0JPWLr",
    "outputId": "77a0b2f1-a1a7-4a6b-8bdc-999fc82e4f93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m35718/35718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 3ms/step - MAE: 1.1419 - loss: 2.4617\n",
      "Epoch 2/10\n",
      "\u001b[1m35718/35718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 3ms/step - MAE: 0.7577 - loss: 0.9498\n",
      "Epoch 3/10\n",
      "\u001b[1m35718/35718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 3ms/step - MAE: 0.7448 - loss: 0.9287\n",
      "Epoch 4/10\n",
      "\u001b[1m35718/35718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 3ms/step - MAE: 0.7357 - loss: 0.9143\n",
      "Epoch 5/10\n",
      "\u001b[1m35718/35718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 3ms/step - MAE: 0.7281 - loss: 0.9027\n",
      "Epoch 6/10\n",
      "\u001b[1m35718/35718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 3ms/step - MAE: 0.7219 - loss: 0.8924\n",
      "Epoch 7/10\n",
      "\u001b[1m35718/35718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 3ms/step - MAE: 0.7168 - loss: 0.8840\n",
      "Epoch 8/10\n",
      "\u001b[1m35718/35718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 3ms/step - MAE: 0.7119 - loss: 0.8754\n",
      "Epoch 9/10\n",
      "\u001b[1m35718/35718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 3ms/step - MAE: 0.7084 - loss: 0.8692\n",
      "Epoch 10/10\n",
      "\u001b[1m35718/35718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 3ms/step - MAE: 0.7043 - loss: 0.8620\n",
      "\u001b[1m71435/71435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 2ms/step - MAE: 0.7600 - loss: 0.9654\n",
      "Test Loss: 0.9659561514854431, Test MAE: 0.7602151036262512\n"
     ]
    }
   ],
   "source": [
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(user_movie_pairs, ratings, test_size=0.2, random_state=42)\n",
    "X_train_user = X_train[:, 0].reshape(-1, 1)\n",
    "X_train_year = X_train[:, 1].reshape(-1, 1)\n",
    "X_train_language = X_train[:, 2].reshape(-1, 1)\n",
    "X_train_budget = X_train[:, 3].reshape(-1, 1)\n",
    "X_train_popularity = X_train[:, 4].reshape(-1, 1)\n",
    "X_train_duration = X_train[:, 5].reshape(-1, 1)\n",
    "X_train_vote_average = X_train[:, 6].reshape(-1, 1)\n",
    "X_train_vote_count = X_train[:, 7].reshape(-1, 1)\n",
    "X_train_revenue = X_train[:, 8].reshape(-1, 1)\n",
    "\n",
    "X_test_user = X_test[:, 0].reshape(-1, 1)\n",
    "X_test_year = X_test[:, 1].reshape(-1, 1)\n",
    "X_test_language = X_test[:, 2].reshape(-1, 1)\n",
    "X_test_budget = X_test[:, 3].reshape(-1, 1)\n",
    "X_test_popularity = X_test[:, 4].reshape(-1, 1)\n",
    "X_test_duration = X_test[:, 5].reshape(-1, 1)\n",
    "X_test_vote_average = X_test[:, 6].reshape(-1, 1)\n",
    "X_test_vote_count = X_test[:, 7].reshape(-1, 1)\n",
    "X_test_revenue = X_test[:, 8].reshape(-1, 1)\n",
    "\n",
    "# Train the Model\n",
    "# Pass all inputs to the model\n",
    "model_final.fit([X_train_user, X_train_year,X_train_language, X_train_budget, X_train_popularity, X_train_duration, X_train_vote_average, X_train_vote_count, X_train_revenue], y_train, epochs=10, batch_size=256)\n",
    "\n",
    "# Evaluate the Model\n",
    "# Pass all inputs during evaluation as well\n",
    "loss, MAE = model_final.evaluate([X_test_user, X_test_year,X_test_language, X_test_budget, X_test_popularity, X_test_duration, X_test_vote_average, X_test_vote_count, X_test_revenue], y_test)\n",
    "print(f\"Test Loss: {loss}, Test MAE: {MAE}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
